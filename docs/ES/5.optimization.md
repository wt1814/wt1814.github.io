

<!-- TOC -->

- [1. ES优化](#1-es优化)
    - [1.1. ES的一些优化点](#11-es的一些优化点)
        - [索引层面调优手段：](#索引层面调优手段)
        - [elasticsearch 索引数据多了怎么办，如何调优，部署](#elasticsearch-索引数据多了怎么办如何调优部署)
        - [1.1.1. Document 模型设计](#111-document-模型设计)
        - [1.1.2. 分片与副本](#112-分片与副本)
        - [1.1.3. 重建索引](#113-重建索引)
        - [1.1.4. Filesystem Cache](#114-filesystem-cache)
        - [1.1.5. 数据预热](#115-数据预热)
        - [1.1.6. 冷热分离](#116-冷热分离)
        - [1.1.7. 分页性能优化](#117-分页性能优化)
        - [1.1.8. JVM调优](#118-jvm调优)
        - [1.1.9. cpu 使用率过高](#119-cpu-使用率过高)
    - [1.2. 使用ES中的一些问题](#12-使用es中的一些问题)
        - [1.2.1. Elasticsearch OOM](#121-elasticsearch-oom)
        - [1.2.2. 聚合数据结果不精确](#122-聚合数据结果不精确)

<!-- /TOC -->

# 1. ES优化  
## 1.1. ES的一些优化点  






### 索引层面调优手段：  
1.1 、设计阶段调优   
1、根据业务增量需求， 采取基于日期模板创建索引， 通过 roll over API 滚动索引；   
2、使用别名进行索引管理；  
3、每天凌晨定时对索引做 force_merge 操作， 以释放空间；  

4、采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink 操 作， 以缩减存储；  
5、采取 curator 进行索引的生命周期管理；  
6、仅针对需要分词的字段， 合理的设置分词器； 
7、Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。  
… ….. 

1.2 、写入调优   
1、写入前副本数设置为 0；   
2、写入前关闭 refresh_interval 设置为-1， 禁用刷新机制；  
3、写入过程中： 采取 bulk 批量写入；  
4、写入后恢复副本数和刷新间隔；  
5、尽量使用自动生成的 id。  

1.3 、查询调优 
1、禁用 wildcard； 
2、禁用批量 terms（ 成百上千的场景）； 
3、充分利用倒排索引机制， 能 keyword 类型尽量 keyword；
4、 数据量大时候， 可以先基于时间敲定索引再检索；
5、设置合理的路由机制。 

1.4 、其他调优 
部署调优， 业务调优等。  

### elasticsearch 索引数据多了怎么办，如何调优，部署

解大数据量的运维能力。  
解答： 索引数据的规划， 应在前期做好规划， 正所谓“ 设计先行， 编码在后” ， 这 样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其 他业务受到影响。  
如何调优， 正如问题 1 所说， 这里细化一下：  
3.1 动态索引层面  
基于模板+时间+rollover api 滚动创建索引， 举例： 设计阶段定义： blog 索引的 模板格式为： blog_index_时间戳的形式，每天递增数据。  
这样做的好处： 不至于数据量激增导致单个索引数据量非常大，接近于上线2的32次幂-1，索引存储达到了TB+甚至更大。  
一旦单个索引很大， 存储等各种风险也随之而来， 所以要提前考虑+及早避免。 

3.2 存储层面  
冷热数据分离存储，热数据（ 比如最近 3 天或者一周的数据）， 其余为冷数据。对于冷 数据不会再写入新数据， 可以考虑定期 force_merge 加 shrink 压缩操作， 节省存储空间 和检索效率。  
3.3 部署层面  
一旦之前没有规划， 这里就属于应急策略。  
 结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意： 如 果之前主节点等规划合理， 不需要重启集群也能完成动态新增的。  


------

### 1.1.1. Document 模型设计  
&emsp; 对于 MySQL，经常有一些复杂的关联查询。ES 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。  
&emsp; 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 ES 中。搜索的时候，就不需要利用 ES 的搜索语法来完成 Join 之类的关联搜索了。  
&emsp; Document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。  
&emsp; ES 能支持的操作就那么多，不要考虑用 ES 做一些它不好操作的事情。如果真的有那种操作，尽量在 Document 模型设计的时候，写入的时候就完成。  
&emsp; 另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。  

### 1.1.2. 分片与副本  
&emsp; 在创建索引的时候，应该创建多少个分片与副本数呢？  
&emsp; 对于副本数，比较好确定，可以根据集群节点的多少与存储空间决定，集群服务器多，并且有足够大多存储空间，可以多设置副本数，一般是 1-3 个副本数，如果集群服务器相对较少并且存储空间没有那么宽松，则可以只设定一份副本以保证容灾（副本数可以动态调整）。  
&emsp; 对于分片数，是比较难确定的。因为一个索引分片数一旦确定，就不能更改，所以在创建索引前，要充分的考虑到，以后创建的索引所存储的数据量，否则创建了不合适的分片数，会对性能造成很大的影响。  
&emsp; 对于分片数的大小，业界一致认为分片数的多少与内存挂钩，认为 1GB 堆内存对应 20-25 个分片，而一个分片的大小不要超过 50G，这样的配置有助于集群的健康。  
&emsp; 查询大量小分片使得每个分片处理数据速度更快了，那是不是分片数越多，查询就越快，ES 性能就越好呢？其实也不是，因为在查询过程中，有一个分片合并的过程，如果分片数不断的增加，合并的时间则会增加，而且随着更多的任务需要按顺序排队和处理，更多的小分片不一定要比查询较小数量的更大的分片更快。如果有多个并发查询，则有很多小碎片也会降低查询吞吐量。  
&emsp; 如果现在场景是分片数不合适了，但是又不知道如何调整，那么有一个好的解决方法就是按照时间创建索引，然后进行通配查询。如果每天的数据量很大，则可以按天创建索引，如果是一个月积累起来导致数据量很大，则可以一个月创建一个索引。如果要对现有索引进行重新分片，则需要重建索引。  


### 1.1.3. 重建索引  
&emsp; 在重建索引之前，首先要考虑一下重建索引的必要性，因为重建索引是非常耗时的。ES 的 reindex api 不会去尝试设置目标索引，不会复制源索引的设置，所以应该在运行_reindex 操作之前设置目标索引，包括设置映射（mapping），分片，副本等。  

&emsp; 第一步，和创建普通索引一样创建新索引。当数据量很大的时候，需要设置刷新时间间隔，把 refresh_intervals 设置为-1，即不刷新,number_of_replicas 副本数设置为 0（因为副本数可以动态调整，这样有助于提升速度）。  

```json
{
	"settings": {

		"number_of_shards": "50",
		"number_of_replicas": "0",
		"index": {
			"refresh_interval": "-1"
		}
	}
	"mappings": {
    }
}
```
&emsp; 第二步，调用 reindex 接口，建议加上 wait_for_completion=false 的参数条件，这样 reindex 将直接返回 taskId。
POST _reindex?wait_for_completion=false  

```json
{
  "source": {
    "index": "old_index",   //原有索引
    "size": 5000            //一个批次处理的数据量
  },
  "dest": {
    "index": "new_index",   //目标索引
  }
}
```

&emsp; 第三步，等待。可以通过 GET _tasks?detailed=true&actions=*reindex 来查询重建的进度。如果要取消 task 则调用_tasks/node_id:task_id/_cancel。  
&emsp; 第四步，删除旧索引，释放磁盘空间。更多细节可以查看 ES 官网的 reindex api。  

&emsp; **如果ES 是实时写入的，该怎么办？**  
&emsp; 这个时候，需要重建索引的时候，在参数里加上上一次重建索引的时间戳，直白的说就是，比如数据是 100G，这时候重建索引了，但是这个 100G 在增加，那么重建索引的时候，需要记录好重建索引的时间戳，记录时间戳的目的是下一次重建索引跑任务的时候不用全部重建，只需要在此时间戳之后的重建就可以，如此迭代，直到新老索引数据量基本一致，把数据流向切换到新索引的名字。  

```json
POST /_reindex
{
    "conflicts": "proceed",          //意思是冲突以旧索引为准，直接跳过冲突，否则会抛出异常，停止task
    "source": {
        "index": "old_index"         //旧索引
        "query": {
            "constant_score" : {
                "filter" : {
                    "range" : {
                        "data_update_time" : {
                            "gte" : 123456789   //reindex开始时刻前的毫秒时间戳
                            }
                        }
                    }
                }
            }
        },
    "dest": {
        "index": "new_index",       //新索引
        "version_type": "external"  //以旧索引的数据为准
        }
}
```

### 1.1.4. Filesystem Cache  
&emsp; 往 ES 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 Filesystem Cache 里面去。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-14.png)  
&emsp; ES 的搜索引擎严重依赖于底层的 Filesystem Cache，如果给 Filesystem Cache 更多的内存，尽量让内存可以容纳所有的 IDX Segment File 索引数据文件，那么搜索的时候就基本都是走内存的，性能会非常高。  
&emsp; 真实的案例：某个公司 ES 节点有 3 台机器，每台机器看起来内存很多 64G，总内存就是 64 * 3 = 192G。  
每台机器给 ES JVM Heap 是 32G，那么剩下来留给 Filesystem Cache 的就是每台机器才 32G，总共集群里给 Filesystem Cache 的就是 32 * 3 = 96G 内存。  
&emsp; 而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，ES 数据量是 1T，那么每台机器的数据量是 300G。这样性能好吗？  
&emsp; Filesystem Cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后执行搜索操作，大部分操作都是走磁盘，性能肯定差。  
&emsp; 归根结底，要让 ES 性能好，最佳的情况下，就是机器的内存，至少可以容纳总数据量的一半。  
&emsp; 根据实践经验，最佳的情况下，是仅仅在 ES 中就存少量的数据，就是要用来搜索的那些索引，如果内存留给 Filesystem Cache 的是 100G，那么就将索引数据控制在 100G 以内。  
&emsp; 这样的话，数据几乎全部走内存来搜索，性能非常之高，一般可以在1秒以内。  
&emsp; 比如说现在有一行数据：id，name，age .... 30 个字段。但是现在搜索，只需要根据 id，name，age 三个字段来搜索。
如果往 ES 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的。  
&emsp; 结果硬是占据了 ES 机器上的 Filesystem Cache 的空间，单条数据的数据量越大，就会导致 Filesystem Cahce 能缓存的数据就越少。  
&emsp; 其实，仅仅写入 ES 中要用来检索的少数几个字段就可以了，比如说就写入 es id，name，age 三个字段。  
&emsp; 然后可以把其他的字段数据存在 MySQL/HBase 里，一般是建议用 ES + HBase 这么一个架构。  
&emsp; HBase 的特点是适用于海量数据的在线存储，就是对 HBase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。  
&emsp; 从 ES 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 HBase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。  
&emsp; 写入 ES 的数据最好小于等于，或者是略微大于 ES 的 Filesystem Cache 的内存容量。  
&emsp; 然后从 ES 检索可能就花费 20ms，然后再根据 ES 返回的 id 去 HBase 里查询，查 20 条数据，可能也就耗费 30ms。  
&emsp; 设置 ES 集群内存的时候，还有一点就是避免交换内存，可以在配置文件中对内存进行锁定，以避免交换内存（也可以在操作系统层面进行关闭内存交换）。对应的参数：bootstrap.mlockall: true。  

### 1.1.5. 数据预热  
&emsp; 假如说，哪怕是按照上述的方案去做了，ES 集群中每个机器写入的数据量还是超过了 Filesystem Cache 一倍。  
&emsp; 比如说写入一台机器 60G 数据，结果 Filesystem Cache 就 30G，还是有 30G 数据留在了磁盘上。  
&emsp; 其实可以做数据预热。举个例子，拿微博来说，可以把一些大 V，平时看的人很多的数据，提前在后台搞个系统。  
&emsp; 每隔一会儿，自己的后台系统去搜索一下热数据，刷到 Filesystem Cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。  
&emsp; 或者是电商，可以将平时查看最多的一些商品，比如说 iPhone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 Filesystem Cache 里去。  
&emsp; 对于那些觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统。  
&emsp; 就是对热数据每隔一段时间，就提前访问一下，让数据进入 Filesystem Cache 里面去。这样下次别人访问的时候，性能一定会好很多。  

### 1.1.6. 冷热分离  
&emsp; ES 可以做类似于 MySQL 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。  
&emsp; 最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让它们留在 Filesystem OS Cache 里，别让冷数据给冲刷掉。  
&emsp; 假设有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 Shard。3 台机器放热数据 Index，另外 3 台机器放冷数据 Index。  
&emsp; 这样的话，大量的时间是在访问热数据 Index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 Filesystem Cache 里面了，就可以确保热数据的访问性能是很高的。  
&emsp; 但是对于冷数据而言，是在别的 Index 里的，跟热数据 Index 不在相同的机器上，大家互相之间都没什么联系了。  
&emsp; 如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。  

### 1.1.7. 分页性能优化  
&emsp; ES深度分页性能很差。  
&emsp; 类似于 App 里的推荐商品不断下拉出来一页一页的；类似于微博中，下拉刷微博，刷出来一页一页的，可以用 Scroll API。
Scroll 会一次性给生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页、下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。  
&emsp; 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。  

### 1.1.8. JVM调优  
&emsp; ElasticSearch是基于Lucene开发的，而Lucene是使用Java开发的，所以也需要针对JVM进行调优。

* 确保堆内存最小值（ Xms ）与最大值（ Xmx ）的大小是相同的，防止程序在运行时改变堆内存大小。  
* Elasticsearch 默认安装后设置的堆内存是 1GB。可通过 ../config/jvm.option 文件进行配置，但是最好不要超过物理内存的50%和超过 32GB。  
* GC 默认采用 CMS 的方式，并发但是有 STW 的问题，可以考虑使用 G1 收集器。  

### 1.1.9. cpu 使用率过高  
&emsp; cpu 使用率，该如何调节呢。cpu 使用率高，有可能是写入导致的，也有可能是查询导致的，那要怎么查看呢？  
&emsp; 可以先通过 GET _nodes/{node}/hot_threads 查看线程栈，查看是哪个线程占用 cpu 高，如果是 elasticsearch[{node}][search][T#10] 则是查询导致的，如果是 elasticsearch[{node}][bulk][T#1] 则是数据写入导致的。  
&emsp; 在实际调优中，cpu 使用率很高，如果不是 SSD，建议把 index.merge.scheduler.max_thread_count: 1 索引 merge 最大线程数设置为 1 个，该参数可以有效调节写入的性能。因为在存储介质上并发写，由于寻址的原因，写入性能不会提升，只会降低。  

## 1.2. 使用ES中的一些问题  
### 1.2.1. Elasticsearch OOM  
<!-- 
公司银子不多，遇到Elasticsearch OOM（内存溢出），除了瞪白眼，还能干啥... 
https://mp.weixin.qq.com/s/W3mSgShSgoqkGz6otZRE5Q
-->
......

### 1.2.2. 聚合数据结果不精确  
<!-- 
 Elasticsearch 聚合数据结果不精确，怎么破？ 
https://mp.weixin.qq.com/s/V4cGqvkQ7-DgeSvPSketgQ
-->



