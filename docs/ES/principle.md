
<!-- TOC -->

- [1. ES机制原理](#1-es机制原理)
    - [1.1. 写数据](#11-写数据)
        - [1.1.1. 写数据过程](#111-写数据过程)
        - [1.1.2. 写数据的底层原理](#112-写数据的底层原理)
    - [1.2. 读数据(根据doc id来查询)](#12-读数据根据doc-id来查询)
    - [1.3. 搜索数据过程](#13-搜索数据过程)
    - [1.4. 删除/更新数据](#14-删除更新数据)

<!-- /TOC -->

&emsp; **<font color = "red">总结：</font>**  
&emsp; **数据先写入内存buffer，然后每隔1s，将数据refresh到os cache，到了os cache数据就能被搜索到(所以说es从写入到能被搜索到，中间有1s的延迟)。每隔5s，将数据写入translog文件(这样如果机器宕机，内存数据全没，最多会有5s的数据丢失)，translog大到一定程度，或者默认每隔30mins，会触发commit操作，将缓冲区的数据都flush到segment file磁盘文件中。数据写入segment file之后，同时就建立好了倒排索引。**  

&emsp; es丢数据？  
&emsp; es第一是准实时的，数据写入1秒后可以搜索到；可能会丢失数据的。有5秒的数据，停留在buffer、translog os cache、segment file os cache中，而不在磁盘上，此时如果宕机，会导致5秒的数据丢失。  

# 1. ES机制原理
<!--
http://blog.itpub.net/31545820/viewspace-2656265/
https://blog.csdn.net/jiaojiao521765146514/article/details/83753215
&emsp; 搜索分为两个过程：  

1. 写数据：当向索引中保存文档时，默认情况下，es会保存两份内容，一份是_source中的数据，另一份则是通过分词、排序等一系列过程生成的倒排索引文件，倒排索引中保存了词项和文档之间的对应关系。  
2. 搜索数据：搜索时，当es接收到用户的搜索请求之后，就会去倒排索引中查询，通过的倒排索引中维护的倒排记录表找到关键词对应的文档集合，然后对文档进行评分、排序、高亮等处理，处理完成后返回文档。  


-->

&emsp; ES作为持久化层，其基本操作就是增删改查。  

## 1.1. 写数据  
### 1.1.1. 写数据过程  
* 客户端选择一个node发送请求过去，这个node就是coordinating node(协调节点)。  
* coordinating node对document进行路由，将请求转发给对应的node(有primary shard)。  
* 实际的node上的primary shard处理请求，然后将数据同步到replica node。  
* coordinating node如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端。  

![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-80.png)  

### 1.1.2. 写数据的底层原理
![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-81.png)  



&emsp; 先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。  
&emsp; 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。  

    操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。  
    为什么叫 es 是准实时的？NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。  
&emsp; 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。  
&emsp; commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。  
&emsp; 这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。  
&emsp; translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。  
&emsp; translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。  

## 1.2. 读数据(根据doc id来查询)  
&emsp; 可以通过doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。  

* 客户端发送请求到任意一个 node，成为 coordinate node 。  
* coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。  
* 接收请求的 node 返回 document 给 coordinate node 。  
* coordinate node 返回 document 给客户端。  

## 1.3. 搜索数据过程  
&emsp; es 最强大的是做全文检索，就是比如有三条数据：

    java真好玩儿啊
    java好难学啊
    j2ee特别牛Copy to clipboardErrorCopied

&emsp; 根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就返回：java真好玩儿啊，java好难学啊。  

* 客户端发送请求到一个 coordinate node 。  
* 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。  
* query phase：每个 shard 将自己的搜索结果(其实就是一些 doc id )返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。  
* fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。  


    写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。

## 1.4. 删除/更新数据  
&emsp; 如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。  
&emsp; 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。  
&emsp; buffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。  

<!-- 
    ES内部是如何运行的？
    主分片和副本分片是如何同步的？
    创建索引的流程是什么样的？
    ES 如何将索引数据分配到不同的分片上的？以及这些索引数据是如何存储的？
    为什么说 ES 是近实时搜索引擎而文档的 CRUD (创建-读取-更新-删除) 操作是实时的？
    以及 Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据？

 1.1. 写索引原理  
&emsp; 下图描述了 3 个节点的集群，共拥有 12 个分片，其中有 4 个主分片(S0、S1、S2、S3)和 8 个副本分片(R0、R1、R2、R3)，每个主分片对应两个副本分片，节点 1 是主节点(Master 节点)负责整个集群的状态。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-10.png)  
&emsp; 写索引是只能写在主分片上，然后同步到副本分片。这里有四个主分片，一条数据 ES 是根据什么规则写到特定分片上的呢？
这条索引数据为什么被写到 S0 上而不写到 S1 或 S2 上？那条数据为什么又被写到 S3 上而不写到 S0 上了？  
&emsp; 实际上，这个过程是根据下面这个公式决定的：  

    shard = hash(routing) % number_of_primary_shards  

&emsp; Routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。Routing 通过 Hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards (主分片的数量)后得到余数。这个在 0 到 number_of_primary_shards-1 之间的余数，就是所寻求的文档所在分片的位置。  
&emsp; 这就解释了为什么要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。  
&emsp; 由于在ES集群中每个节点通过上面的计算公式都知道集群中的文档的存放位置，所以每个节点都有处理读写请求的能力。
在一个写请求被发送到某个节点后，该节点即为前面说过的协调节点，协调节点会根据路由公式计算出需要写到哪个分片上，再将请求转发到该分片的主分片节点上。   
![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-11.png)  
&emsp; 假如此时数据通过路由计算公式取余后得到的值是 shard=hash(routing)%4=0。  
&emsp; 则具体流程如下：  

* 客户端向 ES1 节点(协调节点)发送写请求，通过路由计算公式得到值为 0，则当前数据应被写到主分片 S0 上。  
* ES1 节点将请求转发到 S0 主分片所在的节点 ES3，ES3 接受请求并写入到磁盘。  
* 并发将数据复制到两个副本分片 R0 上，其中通过乐观并发控制数据的冲突。一旦所有的副本分片都报告成功，则节点 ES3 将向协调节点报告成功，协调节点向客户端报告成功。  

 **1.2. 存储原理**  
&emsp; 上面介绍了在 ES 内部索引的写处理流程，这个流程是在 ES 的内存中执行的，数据被分配到特定的分片和副本上之后，最终是存储到磁盘上的，这样在断电的时候就不会丢失数据。  
&emsp; 具体的存储路径可在配置文件 ../config/elasticsearch.yml 中进行设置，默认存储在安装目录的 Data 文件夹下。
建议不要使用默认值，因为若 ES 进行了升级，则有可能导致数据全部丢失：  

    path.data: /path/to/data  //索引数据
    path.logs: /path/to/logs  //日志记录

**1.2.1. 分段存储**  
&emsp; 索引文档以段的形式存储在磁盘上，何为段？索引文件被拆分为多个子文件，则每个子文件叫作段，每一个段本身都是一个倒排索引，并且段具有不变性，一旦索引的数据被写入硬盘，就不可再修改。  
&emsp; 在底层采用了分段的存储模式，使它在读写时几乎完全避免了锁的出现，大大提升了读写性能。段被写入到磁盘后会生成一个提交点，提交点是一个用来记录所有提交后段信息的文件。一个段一旦拥有了提交点，就说明这个段只有读的权限，失去了写的权限。相反，当段在内存中时，就只有写的权限，而不具备读数据的权限，意味着不能被检索。段的概念提出主要是因为：在早期全文检索中为整个文档集合建立了一个很大的倒排索引，并将其写入磁盘中。    
&emsp; 如果索引有更新，就需要重新全量创建一个索引来替换原来的索引。这种方式在数据量很大时效率很低，并且由于创建一次索引的成本很高，所以对数据的更新不能过于频繁，也就不能保证时效性。  

&emsp; 索引文件分段存储并且不可修改，那么新增、更新和删除如何处理呢？  

* 新增，新增很好处理，由于数据是新的，所以只需要对当前文档新增一个段就可以了。
* 删除，由于不可修改，所以对于删除操作，不会把文档从旧的段中移除而是通过新增一个 .del 文件，文件中会列出这些被删除文档的段信息。这个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。  
* 更新，不能修改旧的段来进行反映文档的更新，其实更新相当于是删除和新增这两个动作组成。会将旧的文档在 .del 文件中标记删除，然后文档的新版本被索引到一个新的段中。可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就会被移除。  

&emsp; 段被设定为不可修改具有一定的优势也有一定的缺点，优势主要表现在：  
* 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。  
* 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。  
* 其它缓存(像 Filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。  
* 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和需要被缓存到内存的索引的使用量。  

&emsp; 段的不变性的缺点如下：  
* 当对旧数据进行删除时，旧数据不会马上被删除，而是在 .del 文件中被标记为删除。而旧数据只能等到段更新时才能被移除，这样会造成大量的空间浪费。  
* 若有一条数据频繁的更新，每次更新都是新增新的标记旧的，则会有大量的空间浪费。  
* 每次新增数据时都需要新增一个段来存储数据。当段的数量太多时，对服务器的资源例如文件句柄的消耗会非常大。  
* 在查询的结果中包含所有的结果集，需要排除被标记删除的旧数据，这增加了查询的负担。  

 **1.2.2. 延迟写策略**  
&emsp; 介绍完了存储的形式，那么索引写入到磁盘的过程是怎样的？是否是直接调 Fsync 物理性地写入磁盘？   
&emsp; 为了提升写的性能，ES 并没有每新增一条数据就增加一个段到磁盘上，而是采用延迟写的策略。  
&emsp; 每当有新增的数据时，就将其先写入到内存中，在内存和磁盘之间是文件系统缓存。当达到默认的时间(1 秒钟)或者内存的数据达到一定量时，会触发一次刷新(Refresh)，将内存中的数据生成到一个新的段上并缓存到文件缓存系统 上，稍后再被刷新到磁盘中并生成提交点。这里的内存使用的是 ES 的 JVM 内存，而文件缓存系统使用的是操作系统的内存。  
&emsp; 新的数据会继续的被写入内存，但内存中的数据并不是以段的形式存储的，因此不能提供检索功能。  
&emsp; 由内存刷新到文件缓存系统的时候会生成新的段，并将段打开以供搜索使用，而不需要等到被刷新到磁盘。  
&emsp; 在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 Refresh (即内存刷新到文件缓存系统)。  
&emsp; 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是近实时搜索，因为文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。  
&emsp; 也可以手动触发 Refresh，POST /_refresh 刷新所有索引，POST /nba/_refresh 刷新指定的索引。  

&emsp; 注：尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产>环境下每次索引一个文档都去手动刷新。而且并不是所有的情况都需要每秒刷新。  

&emsp; 可能正在使用 Elasticsearch 索引大量的日志文件， 可能想优化索引速度而不是>近实时搜索。  
&emsp; 这时可以在创建索引时在 Settings 中通过调大 refresh_interval = "30s" 的值 ， 降低每个索引的刷新频率，设值时需要注意后面带上时间单位，否则默认是毫秒。当 refresh_interval=-1 时表示关闭索引的自动刷新。  
&emsp; 虽然通过延时写的策略可以减少数据往磁盘上写的次数提升了整体的写入能力，但是文件缓存系统也是内存空间，属于操作系统的内存，只要是内存都存在断电或异常情况下丢失数据的危险。  
&emsp; 为了避免丢失数据，Elasticsearch 添加了事务日志(Translog)，事务日志记录了所有还没有持久化到磁盘的数据。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-12.png)  

&emsp; 添加了事务日志后整个写索引的流程如上图所示：  

* 一个新文档被索引之后，先被写入到内存中，但是为了防止数据的丢失，会追加一份数据到事务日志中。不断有新的文档被写入到内存，同时也都会记录到事务日志中。这时新数据还不能被检索和查询。  
* 当达到默认的刷新时间或内存中的数据达到一定量后，会触发一次  Refresh，将内存中的数据以一个新段形式刷新到文件缓存系统中并清空内存。这时虽然新段未被提交到磁盘，但是可以提供文档的检索功能且不能被修改。  
* 随着新文档索引不断被写入，当日志数据大小超过 512M 或者时间超过 30 分钟时，会触发一次 Flush。  

&emsp; 内存中的数据被写入到一个新段同时被写入到文件缓存系统，文件系统缓存中数据通过 Fsync 刷新到磁盘中，生成提交点，日志文件被删除，创建一个空的新日志。  
&emsp; 通过这种方式当断电或需要重启时，ES 不仅要根据提交点去加载已经持久化过的段，还需要工具 Translog 里的记录，把未持久化的数据重新持久化到磁盘上，避免了数据丢失的可能。  

 **1.2.3. 段合并**  
&emsp; 由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。  
&emsp; 每一个段都会消耗文件句柄、内存和 CPU 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段然后合并查询结果，所以段越多，搜索也就越慢。  
&emsp; Elasticsearch 通过在后台定期进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。
段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档不会被拷贝到新的大段中。合并的过程中不会中断索引和搜索。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/ES/es-13.png)  
&emsp; 段合并在进行索引和搜索时会自动进行，合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中，这些段既可以是未提交的也可以是已提交的。合并结束后老的段会被删除，新的段被 Flush 到磁盘，同时写入一个包含新段且排除旧的和较小的段的新提交点，新的段被打开可以用来搜索。  
&emsp; 段合并的计算量庞大， 而且还要吃掉大量磁盘 I/O，段合并会拖累写入速率，如果任其发展会影响搜索性能。  
&emsp; Elasticsearch 在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。  

-->

