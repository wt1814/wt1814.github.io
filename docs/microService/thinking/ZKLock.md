
<!-- TOC -->

- [1. 基于ZK实现分布式锁](#1-基于zk实现分布式锁)
    - [1.1. ZK分布式锁实现原理](#11-zk分布式锁实现原理)
        - [1.1.1. 独占锁的实现](#111-独占锁的实现)
        - [1.1.2. 读写锁的实现](#112-读写锁的实现)
            - [1.1.2.1. 读写锁的第一种实现](#1121-读写锁的第一种实现)
            - [1.1.2.2. 读写锁的第二种实现](#1122-读写锁的第二种实现)
    - [1.2. ZK分布式锁实现](#12-zk分布式锁实现)

<!-- /TOC -->

&emsp; **<font color = "red">总结：</font>**  
1. **<font color = "clime">对于ZK来说，实现分布式锁的核心是临时顺序节点和监听机制。</font>**  
2. **ZooKeeper分布式锁的缺点：** 1). 需要依赖zookeeper；2). 性能低。频繁地“写”zookeeper。集群节点数越多，同步越慢，获取锁的过程越慢。  
3. 基于ZooKeeper可以实现分布式的独占锁和读写锁。  
    1. **使用ZK实现分布式独占锁：**<font color="red">在某一节点下，建立临时顺序节点。最小节点获取到锁。非最小节点监听上一节点，上一节点释放锁，唤醒当前节点。</font>  
    2. **使用ZK实现分布式读写锁：<font color = "red">客户端从 Zookeeper 端获取 /share_lock下所有的子节点，并判断自己能否获取锁。</font>**  
        1. **<font color = "red">如果客户端创建的是读锁节点，获取锁的条件（满足其中一个即可）如下：</font>**  

            * 自己创建的节点序号排在所有其他子节点前面  
            * 自己创建的节点前面无写锁节点  
            
        2. **<font color = "red">如果客户端创建的是写锁节点，</font>** 由于写锁具有排他性，所以获取锁的条件要简单一些，只需确定自己创建的锁节点是否排在其他子节点前面即可。  


# 1. 基于ZK实现分布式锁
<!-- 
~~
https://mp.weixin.qq.com/s/9whV1nuwfu2hWt8newteTA 
-->
&emsp; **<font color = "red">参考《ZooKeeper 分布式过程协同技术详解》和《从Paxos到Zookeeper 分布式一致性原理与实践》</font>**  

## 1.1. ZK分布式锁实现原理
&emsp; **<font color = "clime">基于ZooKeeper可以实现分布式的独占锁和读写锁。对于ZK来说，实现分布式锁的核心是临时顺序节点和监听机制。</font>**  
&emsp; **zookeeper分布式锁的缺点：**  

* 需要另外依赖zookeeper，而部分服务是不会使用zookeeper的，增加了系统的复杂性；  
* 相对于redis分布式锁，性能要稍微略差一些。加锁会频繁地“写”zookeeper，增加zookeeper的压力；写zookeeper的时候会在集群进行同步，节点数越多，同步越慢，获取锁的过程越慢。

### 1.1.1. 独占锁的实现  
&emsp; **使用ZK实现分布式独占锁：**  
1. 使用ZK的临时节点和有序节点，每个线程获取锁就是在ZK创建一个临时有序的节点，比如在/lock/目录下。  
2. 创建节点成功后，获取/lock目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点。  
3. 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。  
4. 如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听。  
&emsp; 比如当前线程获取到的节点序号为 /lock/003，然后所有的节点列表为[/lock/001，/lock/002，/lock/003]，则对 /lock/002 这个节点添加一个事件监听器。  
5. 如果锁释放了，会唤醒下一个序号的节点，然后重新执行第 3 步，判断是否自己的节点序号是最小。比如 /lock/001 释放了，/lock/002 监听到时间，此时节点集合为[/lock/002，/lock/003]，则 /lock/002 为最小序号节点，获取到锁。  

&emsp; <font color="red">一句话概述：在某一节点下，建立临时顺序节点。最小节点获取到锁。非最小节点监听上一节点，上一节点释放锁，唤醒当前节点。</font>

&emsp; 整个过程如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-14.png)  

----
* 获取锁：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-15.png)  
    1. 多个客户端去创建节点时，ZooKeeper会自动根据请求到达的时间顺序对节点进行编号；  
    2. 每个客户端需要判断自己所创建的节点是否是所有子节点中最小的一个，如果是则说明获取到锁，可以直接执行方法，否则监听上一个节点等待锁。  


* 释放锁：  

    1. 获取到锁的客户端执行结束后删除该节点，此时会触发下一个客户端的监听器去获取锁。  
    ![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-16.png)  
    &emsp; 例如：持有/lock/000的客户端在执行结束后删除该节点，持有/lock/001的客户端会收到通知。所有争夺分布式锁的客户端会按照节点的创建顺序执行任务，形成一个等待队列。  
    2. 如果客户端在创建节点后由于某些原因断开了与ZooKeeper的连接，则该客户端的Session失效时会删除其创建的节点，并进行通知。  
    ![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-17.png)  
    &emsp; 例如：持有/lock/000的客户端还在执行方法，持有/lock/001的客户端突然断开连接，为了不让后面的节点收到错误的通知顺序，要尽可能保证在该客户端之前获取锁的所有客户端都能执行完成，需适当加大SessionTimeOut的值来延长节点的存活时间。  

### 1.1.2. 读写锁的实现  
&emsp; 读写锁包含一个读锁和写锁，操作O1对资源R1加读锁，且获得了锁，其他操作可同时对资源R1设置读锁，进行共享读操作。如果操作O1对资源R1加写锁，且获得了锁，其他操作再对资源R1设置不同类型的锁都会被阻塞。总结来说，读锁具有共享性，而写锁具有排他性。那么在 Zookeeper 中，可以用怎样的节点结构实现上面的操作呢？  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-37.png)  
&emsp; 在Zookeeper中，由于读写锁和独占锁的节点结构不同，读写锁的客户端不用再去竞争创建lock节点。所以在一开始，所有的客户端都会创建自己的锁节点。之后， **<font color = "red">客户端从 Zookeeper 端获取 /share_lock下所有的子节点，并判断自己能否获取锁。</font>**  
&emsp; **<font color = "red">如果客户端创建的是读锁节点，获取锁的条件（满足其中一个即可）如下：</font>**  
1. 自己创建的节点序号排在所有其他子节点前面  
2. 自己创建的节点前面无写锁节点  

&emsp; **<font color = "red">如果客户端创建的是写锁节点，</font>** 由于写锁具有排他性，所以获取锁的条件要简单一些，只需确定自己创建的锁节点是否排在其他子节点前面即可。  

&emsp; 不同于独占锁，读写锁的实现稍微复杂一下。读写锁有两种实现方式，各有异同，接下来就来说说这两种实现方式。  

#### 1.1.2.1. 读写锁的第一种实现  
&emsp; **<font color = "red">第一种实现是对 /sharelock 节点设置 watcher，当 /sharelock 下的子节点被删除时，未获取锁的客户端收到 /share_lock 子节点变动的通知。在收到通知后，客户端重新判断自己创建的子节点是否可以获取锁，如果失败，再次等待通知。</font>** 详细流程如下：  
1. 所有客户端创建自己的锁节点。  
2. 从 Zookeeper 端获取/sharelock下所有的子节点，并对/sharelock节点设置 watcher。  
3. 判断自己创建的锁节点是否可以获取锁，如果可以，持有锁。否则继续等待。  
4. 持有锁的客户端删除自己的锁节点，其他客户端收到 /share_lock 子节点变动的通知。  
5. 重复步骤2、3、4，直至无客户端在等待获取锁了。  

&emsp; 上述步骤对于的流程图如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-38.png)  
&emsp; 上面获取读写锁流程并不复杂，但却存在性能问题。以图3所示锁节点结构为例，第一个锁节点 host1-W-0000000001 被移除后，Zookeeper 会将 /share_lock 子节点变动的通知分发给所有的客户端。但实际上，该子节点变动通知除了能影响 host2-R-0000000002 节点对应的客户端外，分发给其他客户端则是在做无用功，因为其他客户端即使获取了通知也无法获取锁。所以这里需要做一些优化，优化措施是让客户端只在自己关心的节点被删除时，再去获取锁。  

#### 1.1.2.2. 读写锁的第二种实现  
&emsp; 在了解读写锁第一种实现的弊端后，针对这一实现进行优化。这里客户端不再对 /share_lock 节点进行监视，而只对自己关心的节点进行监视。还是以图3的锁节点结构进行举例说明，host2-R-0000000002 对应的客户端 C2 只需监视 host1-W-0000000001 节点是否被删除即可。而 host3-W-0000000003 对应的客户端 C3 只需监视 host2-R-0000000002 节点是否被删除即可，只有 host2-R-0000000002 节点被删除，客户端 C3 才能获取锁。而 host1-W-0000000001 节点被删除时，产生的通知对于客户端 C3 来说是无用的，即使客户端 C3 响应了通知也没法获取锁。这里总结一下，不同客户端关心的锁节点是不同的。如果客户端创建的是读锁节点，那么客户端只需找出比读锁节点序号小的最后一个的写锁节点，并设置 watcher 即可。而如果是写锁节点，则更简单，客户端仅需对该节点的上一个节点设置 watcher 即可。详细的流程如下：  
1. 所有客户端创建自己的锁节点。
2. 从Zookeeper端获取/share_lock下所有的子节点。
3. 判断自己创建的锁节点是否可以获取锁，如果可以，持有锁。否则对自己关心的锁节点设置watcher。
4. 持有锁的客户端删除自己的锁节点，某个客户端收到该节点被删除的通知，并获取锁。
5. 重复步骤4，直至无客户端在等待获取锁了。

&emsp; 上述步骤对于的流程图如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/problems/problem-39.png)  

## 1.2. ZK分布式锁实现  
&emsp; ZK分布式锁一般使用框架Curator实现。  
