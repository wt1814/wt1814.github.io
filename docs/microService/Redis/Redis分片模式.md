---
title: Redis分片
date: 2020-05-17 00:00:00
tags:
    - Redis
---

<!-- TOC -->

- [1. 分片模式](#1-分片模式)
    - [1.1. 基于客户端分片](#11-基于客户端分片)
    - [1.2. 基于代理服务器分片](#12-基于代理服务器分片)
    - [1.3. Redis Cluster集群](#13-redis-cluster集群)
        - [1.3.1. 架构](#131-架构)
        - [1.3.2. 部署](#132-部署)
        - [1.3.3. 原理](#133-原理)
            - [1.3.3.1. 数据分布](#1331-数据分布)
                - [1.3.3.1.1. Redis数据分区](#13311-redis数据分区)
                - [1.3.3.1.2. 集群功能限制](#13312-集群功能限制)
            - [1.3.3.2. 集群伸缩](#1332-集群伸缩)
                - [1.3.3.2.1. 伸缩原理](#13321-伸缩原理)
                - [1.3.3.2.2. 扩容集群](#13322-扩容集群)
                - [1.3.3.2.3. 收缩集群](#13323-收缩集群)
            - [1.3.3.3. 请求路由，客户端重定向](#1333-请求路由客户端重定向)
                - [1.3.3.3.1. 请求重定向](#13331-请求重定向)
                - [1.3.3.3.2. Smart客户端](#13332-smart客户端)
                - [1.3.3.3.3. ASK重定向](#13333-ask重定向)
            - [1.3.3.4. 故障转移](#1334-故障转移)
                - [1.3.3.4.1. 故障发现](#13341-故障发现)
                - [1.3.3.4.2. 故障恢复](#13342-故障恢复)
        - [1.3.4. 集群运维](#134-集群运维)
            - [1.3.4.1. 集群倾斜](#1341-集群倾斜)

<!-- /TOC -->

![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-74.png)  

# 1. 分片模式  
&emsp; **<font color = "lime">分片(sharding)是将数据拆分到多个Redis实例的过程，这样每个实例将只包含所有键的子集</font>**，这种方法在解决某些问题时可以获得线性级别的性能提升。  
&emsp; **<font color = "red">根据执行分片的位置，可以分为三种分片方式：</font>**  

* 客户端分片：在客户端实现相关的逻辑，例如用取模或者一致性哈希对 key 进行分片，查询和修改都先判断 key 的路由。  
* 代理分片：把做分片处理的逻辑抽取出来，运行一个独立的代理服务，客户端连接到 这个代理服务，代理服务做请求的转发。  
* 服务器分片：官方Redis Cluster。  

## 1.1. 基于客户端分片  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-18.png)  
&emsp; Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是基于哈希算法，根据Redis数据的key的哈希值对数据进行分片，将数据映射到各自节点上。  
&emsp; 优点在于实现简单，缺点在于当Redis集群调整，每个客户端都需要更新调整。  
&emsp; **在redis3.0版本之前的版本，可以通过redis客户端做sharding分片，比如jedis实现的ShardedJedisPool。**  

## 1.2. 基于代理服务器分片
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-19.png)  
&emsp; 客户端发送请求到独立部署代理组件，代理组件解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端。  
&emsp; 优点在于透明接入，容易集群扩展，缺点在于多了一层代理转发，性能有所损耗。  
&emsp; 典型的代理分区方案有 Twitter 开源的 Twemproxy 和国内的豌豆荚开源的 Codis。  

## 1.3. Redis Cluster集群  
&emsp; Redis Cluster是在3.0版本正式推出的高可用集群方案。  
&emsp; <font color = "red">Redis Cluster相比Redis Sentinel，Redis Cluster方案不需要额外部署Sentinel集群，而是通过集群内部通信实现集群监控，故障时主从切换；同时，支持内部基于哈希槽实现数据分片，支持动态水平扩容。</font>  
&emsp; Redis Cluster相比Codis，它是去中心化的，客户端可以连接到任意一个可用节点。  

&emsp; 特点：  
*  高性能  
    * 采用了异步复制机制，向某个节点写入数据时，无需等待其它节点的写数据响应。  
    * 去中心化，无中心代理节点，每个节点保存数据和整个集群状态，每个节点都和其他所有节点连接，将客户端直接重定向到拥有数据的节点。  
    * 对于N个Master节点的Cluster ，整体性能理论上相当于单个Redis的性能的N倍。  
* 高可用
    * 采用了主从复制的机制，Master节点失效时Slave节点自动提升为Master节点。如果Cluster中有N个Master节点，每个Master拥有1个Slave节点，那么这个Cluster的失效概率为1/(2*N-1)，可用概率为 1-1/(2*N-1)。  
* 高可扩展  
    * 可支持多达1000个服务节点。随时可以向 Cluster 中添加新节点，或者删除现有节点。Cluster中每个节点都与其它节点建立了相互连接。  

&emsp; 优势：  
1. 无中心架构。  
2. 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。  
3. 可扩展性，可线性扩展到 1000 个节点（官方推荐不超过 1000 个），节点可动 态添加或删除。  
4. 高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副 本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制 完成 Slave 到 Master 的角色提升。   
5. 降低运维成本，提高系统的扩展性和可用性。  

&emsp; 不足：  
1. Client 实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时 更新，提高了开发难度，客户端的不成熟影响业务的稳定性。   
2. 节点会因为某些原因发生阻塞（阻塞时间大于 clutser-node-timeout），被判断下线，这种 failover 是没有必要的。   
3. 数据通过异步复制，不保证数据的强一致性。  
4. 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。  

### 1.3.1. 架构  
&emsp; 服务器节点：3主3从，最少6个节点。其 Redis Cluster架构图如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-30.png)  

### 1.3.2. 部署  
...

### 1.3.3. 原理  
#### 1.3.3.1. 数据分布  
##### 1.3.3.1.1. Redis数据分区  
&emsp; Redis集群并没有使用传统的一致性哈希来分配数据，而是采用哈希槽(hash slot)的方式来分配数据。Redis Cluster默认分配了16384（2∧14）个slot，set一个key时，redis采用CRC16算法进行键-槽（key->slot）之间的映射：HASH_SLOT（key）= CRC16(key) % 16384。其中 CRC16(key) 语句用于计算键key的CRC16校验和。key经过公式计算后得到所对应的哈希槽，而哈希槽被某个主节点管理，从而确定key在哪个主节点上存取。  

&emsp; <font color = "red">分区优点：RedisCluster将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞</font>，所以无论是添加新节点还是移除已存在节点，又或者改变某个节点包含的哈希槽数量，都不会造成集群下线。从而保证集群的可用性，使得用户可以很容易地向集群中添加或者删除节点。  
&emsp; 示例：现在三个主节点分别是：A, B, C三个节点，它们可以是一台机器上的三个端口，也可以是三台不同的服务器。那么，采用哈希槽 (hash slot)的方式来分配16384个slot 的话，它们三个节点分别承担的slot区间是：节点A覆盖0－5460；节点B覆盖5461－10922；节点C覆盖10923－16383。  
&emsp; 获取数据：如果存入一个值，按照redis cluster哈希槽的算法CRC16('key')384 = 6782。 那么就会把这个key 的存储分配到B上了。同样，当连接(A,B,C)任何一个节点想获取'key'这个key时，也会这样的算法，然后内部跳转到B节点上获取数据。  
&emsp; 新增一个主节点：新增一个节点D，redis cluster的这种做法是从各个节点的前面各拿取一部分slot到D上。大致就会变成这样：节点A覆盖1365-5460；节点B覆盖6827-10922；节点C覆盖12288-16383；节点D覆盖0-1364,5461-6826,10923-12287。  
&emsp; 同样删除一个节点也是类似，移动完成后就可以删除这个节点了。  

##### 1.3.3.1.2. 集群功能限制  
&emsp; **<font color = "red">Redis集群相对单机在功能上存在一些限制</font>**，需要开发人员提前了解， 在使用时做好规避。限制如下：  
1. <font color = "red">key批量操作支持有限。</font>如mset、mget，目前只支持具有相同slot值的 key执行批量操作。对于映射为不同slot值的key由于执行mget、mget等操作可 能存在于多个节点上因此不被支持。  
2. key事务操作支持有限。同理只支持多key在同一节点上的事务操 作，当多个key分布在不同的节点上时无法使用事务功能。   
3. key作为数据分区的最小粒度，因此不能将一个大的键值对象如 hash、list等映射到不同的节点。   
4. 不支持多数据库空间。单机下的Redis可以支持16个数据库，集群模 式下只能使用一个数据库空间，即db0。  
5. 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。  

#### 1.3.3.2. 集群伸缩  
##### 1.3.3.2.1. 伸缩原理  
&emsp; Redis集群提供了灵活的节点扩容和收缩方案。在不影响集群对外服务 的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容。  
&emsp; Redis集群可以实现对节点的灵活上下线控制。其中原理可抽象为槽和对应数据在不同节点之间灵活移动。    

##### 1.3.3.2.2. 扩容集群  
&emsp; 扩容是分布式存储最常见的需求，Redis集群扩容操作可分为如下步骤：
1. 准备新节点。 
2. 加入集群。 
3. 迁移槽和数据。

##### 1.3.3.2.3. 收缩集群  
......

#### 1.3.3.3. 请求路由，客户端重定向  
&emsp; Redis集群对客户端通信协议做了比较大的修改， 为了追求性能最大化，并没有采用代理的方式而是采用客户端直连节点的方式。  

##### 1.3.3.3.1. 请求重定向  
&emsp; 在集群模式下，Redis接收任何键相关命令时首先计算键对应的槽，再 根据槽找出所对应的节点，如果节点是自身，则处理键命令；否则回复MOVED重定向错误，通知客户端请求正确的节点。这个过程称为MOVED重定向。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-48.png)  

##### 1.3.3.3.2. Smart客户端  
&emsp; 大多数开发语言的Redis客户端都采用Smart客户端支持集群协议，客户端如何选择见：http://redis.io/clients ，从中找出符合自己要求的客户端类 库。Smart客户端通过在内部维护slot→node的映射关系，本地就可实现键到 节点的查找，从而保证IO效率的最大化，而MOVED重定向负责协助Smart客 户端更新slot→node映射。  
&emsp; Jedis为Redis Cluster提供了Smart客户端，对应的类是JedisCluster。  

##### 1.3.3.3.3. ASK重定向  
&emsp; Redis集群支持在线迁移槽（slot）和数据来完成水平伸缩，当slot对应 的数据从源节点到目标节点迁移过程中，客户端需要做到智能识别，保证键 命令可正常执行。例如当一个slot数据从源节点迁移到目标节点时，期间可 能出现一部分数据在源节点，而另一部分在目标节点。  
&emsp; 当出现上述情况时，客户端键命令执行流程将发生变化，如下所示： 
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-49.png)   
1. 客户端根据本地slots缓存发送命令到源节点，如果存在键对象则直 接执行并返回结果给客户端。  
2. 如果键对象不存在，则可能存在于目标节点，这时源节点会回复ASK重定向异常。格式如下：（error）ASK{slot}{targetIP}：{targetPort}。   
3. 客户端从ASK重定向异常提取出目标节点信息，发送asking命令到目 标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返 回不存在信息。   

&emsp; ASK与MOVED虽然都是对客户端的重定向控制，但是有着本质区别。 ASK重定向说明集群正在进行slot数据迁移，客户端无法知道什么时候迁移 完成，因此只能是临时性的重定向，客户端不会更新slots缓存。但是 MOVED重定向说明键对应的槽已经明确指定到新的节点，因此需要更新 slots缓存。  

#### 1.3.3.4. 故障转移  
##### 1.3.3.4.1. 故障发现  
&emsp; 当集群内某个节点出现问题时，需要通过一种健壮的方式保证识别出节点是否发生了故障。<font color = "red">Redis集群内节点通过ping/pong消息实现节点通信，消息不但可以传播节点槽信息，还可以传播其他状态如：主从状态、节点故障等。</font>因此故障发现也是通过消息传播机制实现的，主要环节包括：主观下线（pfail）和客观下线（fail）。   

* 主观下线：指某个节点认为另一个节点不可用，即下线状态，这个状态并不是最终的故障判定，只能代表一个节点的意见，可能存在误判情况。   
* 客观下线：指标记一个节点真正的下线，集群内多个节点都认为该节点不可用，从而达成共识的结果。如果是持有槽的主节点故障，需要为该节点进行故障转移。  

##### 1.3.3.4.2. 故障恢复  
&emsp; 故障节点变为客观下线后，如果下线节点是持有槽的主节点则需要在它的从节点中选出一个替换它，从而保证集群的高可用。下线主节点的所有从节点承担故障恢复的义务，当从节点通过内部定时任务发现自身复制的主节点进入客观下线时，将会触发故障恢复流程。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-50.png)  

### 1.3.4. 集群运维  
#### 1.3.4.1. 集群倾斜  

&emsp; 集群倾斜指不同节点之间数据量和请求量出现明显差异，这种情况将加大负载均衡和开发运维的难度。因此需要理解哪些原因会造成集群倾斜，从而避免这一问题。   
1. 数据倾斜  
&emsp; 数据倾斜主要分为以下几种： 

    * 节点和槽分配严重不均。 
    * 不同槽对应键数量差异过大。 
    * 集合对象包含大量元素。 
    * 内存相关配置不一致。

2. 请求倾斜  
&emsp; 集群内特定节点请求量/流量过大将导致节点之间负载不均，影响集群均衡和运维成本。常出现在热点键场景，当键命令消耗较低时如小对象的get、set、incr等，即使请求量差异较大一般也不会产生负载严重不均。但是当热点键对应高算法复杂度的命令或者是大对象操作如hgetall、smembers等，会导致对应节点负载过高的情况。避免方式如下：  

    * 合理设计键，热点大集合对象做拆分或使用hmget替代hgetall避免整体读取。
    * 不要使用热键作为hash_tag，避免映射到同一槽。 
    * 对于一致性要求不高的场景，客户端可使用本地缓存减少热键调用。   



<!-- 

 5. 集群优雅扩容  
&emsp; 程序如果能够知道Redis集群地址产生了变化，重新设置一下jedis客户端的连接配置。现在的问题就是如何知道Redis集群地址发生了改变？
可以采用把Redis的集群地址配置在zookeeper中，应用在启动的时候，获取zk上的集群地址的值，进行初始化。如果想要改变集群地址，要在zk上面进行设置。  
&emsp; zk重要的特性就是监听特性，节点发生变化，就会立刻把变化发送给应用，从而应用获取到值，重新设置jedis客户端连接。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-26.png)  
-->



