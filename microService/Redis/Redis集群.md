---
title: Redis集群
date: 2020-05-17 00:00:00
tags:
    - Redis
---


<!-- TOC -->

- [1. 单机](#1-单机)
- [2. 主从复制模式：](#2-主从复制模式)
    - [2.1. 主从复制的作用](#21-主从复制的作用)
    - [2.2. 主从复制启用：](#22-主从复制启用)
    - [2.3. 复制流程](#23-复制流程)
        - [2.3.1. 复制流程概述](#231-复制流程概述)
        - [2.3.2. 数据同步阶段两种复制方式](#232-数据同步阶段两种复制方式)
            - [2.3.2.1. 全量复制](#2321-全量复制)
            - [2.3.2.2. 部分复制](#2322-部分复制)
                - [2.3.2.2.1. 基本概念](#23221-基本概念)
                    - [2.3.2.2.1.1. 复制偏移量](#232211-复制偏移量)
                    - [2.3.2.2.1.2. 复制积压缓冲区](#232212-复制积压缓冲区)
                    - [2.3.2.2.1.3. 运行 ID（runid）](#232213-运行-idrunid)
                - [2.3.2.2.2. 部分复制流程](#23222-部分复制流程)
            - [2.3.2.3. psync 命令的执行过程详解](#2323-psync-命令的执行过程详解)
    - [2.4. 主从复制应用与问题](#24-主从复制应用与问题)
        - [2.4.1. 读写分离](#241-读写分离)
            - [2.4.1.1. 数据延迟](#2411-数据延迟)
            - [2.4.1.2. 读到过期数据](#2412-读到过期数据)
            - [2.4.1.3. 从节点故障](#2413-从节点故障)
        - [2.4.2. 规避全量复制](#242-规避全量复制)
            - [2.4.2.1. 第一次复制](#2421-第一次复制)
            - [2.4.2.2. 节点运行 ID 不匹配](#2422-节点运行-id-不匹配)
            - [2.4.2.3. 复制偏移量 offset 不在复制积压缓冲区中](#2423-复制偏移量-offset-不在复制积压缓冲区中)
        - [2.4.3. 规避复制风暴](#243-规避复制风暴)
            - [2.4.3.1. 单一主节点](#2431-单一主节点)
            - [2.4.3.2. 单一机器](#2432-单一机器)
- [3. 哨兵模式](#3-哨兵模式)
    - [3.1. 架构](#31-架构)
    - [3.2. 部署及故障转移演示](#32-部署及故障转移演示)
    - [3.3. 哨兵原理](#33-哨兵原理)
        - [3.3.1. 心跳检查](#331-心跳检查)
        - [3.3.2. 主观下线、客观下线](#332-主观下线客观下线)
            - [3.3.2.1. 主观下线](#3321-主观下线)
            - [3.3.2.2. 客观下线](#3322-客观下线)
        - [3.3.3. Sentinel 选举](#333-sentinel-选举)
        - [3.3.4. 故障转移](#334-故障转移)
- [4. 分片模式](#4-分片模式)
    - [4.1. 基于客户端分片](#41-基于客户端分片)
    - [4.2. 基于代理服务器分片](#42-基于代理服务器分片)
    - [4.3. Redis Cluster集群](#43-redis-cluster集群)
        - [4.3.1. 架构](#431-架构)
        - [4.3.2. 部署](#432-部署)
        - [4.3.3. 原理](#433-原理)
            - [4.3.3.1. 数据分布](#4331-数据分布)
                - [4.3.3.1.1. Redis数据分区](#43311-redis数据分区)
                - [4.3.3.1.2. 集群功能限制](#43312-集群功能限制)
            - [4.3.3.2. 集群伸缩](#4332-集群伸缩)
                - [4.3.3.2.1. 伸缩原理](#43321-伸缩原理)
                - [4.3.3.2.2. 扩容集群](#43322-扩容集群)
                - [4.3.3.2.3. 收缩集群](#43323-收缩集群)
            - [4.3.3.3. 请求路由，客户端重定向](#4333-请求路由客户端重定向)
                - [4.3.3.3.1. 请求重定向](#43331-请求重定向)
                - [4.3.3.3.2. Smart客户端](#43332-smart客户端)
                - [4.3.3.3.3. ASK重定向](#43333-ask重定向)
            - [4.3.3.4. 故障转移](#4334-故障转移)
                - [4.3.3.4.1. 故障发现](#43341-故障发现)
                - [4.3.3.4.2. 故障恢复](#43342-故障恢复)
        - [4.3.4. 集群运维](#434-集群运维)
            - [4.3.4.1. 集群倾斜](#4341-集群倾斜)

<!-- /TOC -->

![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-51.png)  

&emsp; Redis部署方式：单机、主从复制模式、哨兵模式、分片模式（包含客户端分片、代理分片、服务器分片即Redis Cluster）。    
&emsp; <font color="red">参考《Redis开发与运维》</font>  

# 1. 单机  
&emsp; Redis 单机部署一般存在如下几个问题：  

* 机器故障，导致 Redis 不可用，数据丢失  
* 容量瓶颈：容量不能水平扩展  
* QPS 瓶颈：一台机器的处理能力、网络宽带总是有限的，如果能够划分一些流量到其他机器，可以有效解决 QPS 问题  



# 2. 主从复制模式：  
## 2.1. 主从复制的作用  

* 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。  
* 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。  
* 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。  
* 读写分离：可以用于实现读写分离，主库写、从库读，读写分离不仅可以提高服务器的负载能力，同时可根据需求的变化，改变从库的数量；  
* 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。  

## 2.2. 主从复制启用：  
&emsp; 临时配置：redis-cli进入redis从节点后，使用 --slaveof [masterIP] [masterPort]  
&emsp; 永久配置：进入从节点的配置文件redis.conf，增加slaveof [masterIP] [masterPort]  

## 2.3. 复制流程  

### 2.3.1. 复制流程概述  
&emsp; 主从复制过程大体可以分为3个阶段：连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段。  
&emsp; 在从节点执行 slaveof 命令后，复制过程便开始运作，从下图中可以看出复制过程大致分为6个过程。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-20.png)  

    连接建立阶段  

&emsp; 1）保存主节点（master）信息。  
&emsp; 执行 slaveof 后 Redis 会打印如下日志：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-21.png)  

&emsp; 2）从节点（slave）内部通过每秒运行的定时任务维护复制相关逻辑，当定时任务发现存在新的主节点后，会尝试与该节点建立网络连接  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-22.png)  
&emsp; 从节点与主节点建立网络连接  
&emsp; 从节点会建立一个 socket 套接字，从节点建立了一个端口为51234的套接字，专门用于接受主节点发送的复制命令。从节点连接成功后打印如下日志：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-23.png)  

&emsp; 如果从节点无法建立连接，定时任务会无限重试直到连接成功或者执行 slaveof no one 取消复制  

&emsp; 关于连接失败，可以在从节点执行 info replication 查看 master_link_down_since_seconds 指标，它会记录与主节点连接失败的系统时间。从节点连接主节点失败时也会每秒打印如下日志，方便发现问题：  

```
# Error condition on socket for SYNC: {socket_error_reason}
```

&emsp; 3）发送 ping 命令。  
&emsp; 连接建立成功后从节点发送 ping 请求进行首次通信，ping 请求主要目的如下：  
* 检测主从之间网络套接字是否可用。  
* 检测主节点当前是否可接受处理命令。  

&emsp; 如果发送 ping 命令后，从节点没有收到主节点的 pong 回复或者超时，比如网络超时或者主节点正在阻塞无法响应命令，从节点会断开复制连接，下次定时任务会发起重连。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-24.png)  
&emsp; 从节点发送的 ping 命令成功返回，Redis 打印如下日志，并继续后续复制流程：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-25.png)  
&emsp; 4）权限验证。如果主节点设置了 requirepass 参数，则需要密码验证，从节点必须配置 masterauth 参数保证与主节点相同的密码才能通过验证；如果验证失败复制将终止，从节点重新发起复制流程。  

    数据同步阶段  

&emsp; 5）同步数据集。主从复制连接正常通信后，对于首次建立复制的场景，主节点会把持有的数据全部发送给从节点，这部分操作是耗时最长的步骤。  

    命令传播阶段  
    
&emsp; 6）命令持续复制。当主节点把当前的数据同步给从节点后，便完成了复制的建立流程。接下来主节点会持续地把写命令发送给从节点，保证主从数据一致性。  

### 2.3.2. 数据同步阶段两种复制方式

&emsp; 主从节点在数据同步阶段，主节点会根据当前状态的不同执行不同复制操作，包括：全量复制和部分复制。  
&emsp; redis 2.8之前使用sync [runId] [offset]同步命令，redis2.8之后使用psync [runId] [offset]命令。两者不同在于，sync命令仅支持全量复制过程，psync支持全量和部分复制。  

* 全量复制：用于首次复制或者其他不能进行部分复制的情况。全量复制是一个非常重的操作，一般都要规避它。  
* 部分复制：用于从节点短暂中断的情况（网络中断、短暂的服务宕机）。部分复制是一个非常轻量级的操作，因为它只需要将中断期间的命令同步给从节点即可，相比于全量复制，它显得更加高效。  

#### 2.3.2.1. 全量复制  
&emsp; 全量复制的流程图如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-40.png)  

&emsp; 1. 由于是第一次进行数据同步，从节点并不知道主节点的 runid，所以发送 psync ? -1  
&emsp; 2. 主节点接收从节点的命令后，判定是进行全量复制，所以回复 +FULLRESYNC ，同时也会将自身的 runid 和 偏移量发送给从节点，响应为 +FULLRESYNC{runid}{offset}  
&emsp; 3. 从节点接受主节点的响应后，会保存主节点的 runid 和 偏移量 offset。打印日志如下：  

    62760:S 16 May 2019 21:24:36.818 * Trying a partial resynchronization (request cf2836e6d8f3628c81b3ebb36fea4410f21f05b0:1).
    62760:S 16 May 2019 21:24:36.820 * Full resync from master: a7113788690a86b166cf978b874b3c6056167b54:0

&emsp; 从节点尝试部分复制，请求节点的runid 为 cf2836e6d8f3628c81b3ebb36fea4410f21f05b0，offset：1，但是主节点告知从节点是全量复制，runid：a7113788690a86b166cf978b874b3c6056167b54，offset：1。  
&emsp; 4. 主节点响应从节点命令后，会执行 bgsave，将生成的 RDB 文件保存在本地。打印日志如下：  

    62743:M 16 May 2019 21:24:36.819 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'cf2836e6d8f3628c81b3ebb36fea4410f21f05b0', my replication IDs are '0156844c881cf978fa35de7deeb9f85ef7cd1b0e' and '0000000000000000000000000000000000000000')
    62743:M 16 May 2019 21:24:36.819 * Starting BGSAVE for SYNC with target: disk
    62743:M 16 May 2019 21:24:36.820 * Background saving started by pid 62770
    62770:C 16 May 2019 21:24:36.821 * DB saved on disk
    62743:M 16 May 2019 21:24:36.884 * Background saving terminated with success

&emsp; 主节点接受从节点的部分请求，但是 runid 不一致，进行全量复制，返回 +FULLRESYNC，并将自身的 runid 和 offset 返回给从节点。  
&emsp; 5. 主节点将生成的 RDB 文件发送给从节点，从节点接收后保存在本地直接将其作为数据文件，如果从节点本地有 RDB 文件，则从节点会先清空 RDB 文件。从节点打印日志如下：

    62760:S 16 May 2019 21:24:36.885 * MASTER <-> REPLICA sync: receiving 234 bytes from master
    62760:S 16 May 2019 21:24:36.886 * MASTER <-> REPLICA sync: Flushing old data
    62760:S 16 May 2019 21:24:36.886 * MASTER <-> REPLICA sync: Loading DB in memory
    62760:S 16 May 2019 21:24:36.886 * MASTER <-> REPLICA sync: Finished with success

&emsp; 6. 如果从节点还开启了 AOF，则还会进行 AOF 重写，日志如下：  

    62760:S 16 May 2019 21:24:36.887 * Background append only file rewriting started by pid 62771
    62760:S 16 May 2019 21:24:36.911 * AOF rewrite child asks to stop sending diffs.
    62771:C 16 May 2019 21:24:36.912 * Parent agreed to stop sending diffs. Finalizing AOF...
    62771:C 16 May 2019 21:24:36.913 * Concatenating 0.00 MB of AOF diff received from parent.
    62771:C 16 May 2019 21:24:36.913 * SYNC append only file rewrite performed
    62760:S 16 May 2019 21:24:36.918 * Background AOF rewrite terminated with success
    62760:S 16 May 2019 21:24:36.919 * Residual parent diff successfully flushed to the rewritten AOF (0.00 MB)
    62760:S 16 May 2019 21:24:36.919 * Background AOF rewrite finished successfully

&emsp; 从上面过程可以看出，全量复制是一个非常重的操作过程，它的开销主要有：  

* 主节点执行 bgsave 过程，在持久化那篇博客中我们知道该过程是非常消耗 CPU、内存(页表复制)、硬盘 IO 的  
* 主节点发送 RDB 给从节点的网络开销  
* 从节点清空 RBD 文件（如果有）和加载 RDB 文件，同时该过程是阻塞的，无法响应客户端命令  
* 如果从节点开启了 AOF，则还有 bgrewriteaof 的开销  
所以，需要尽可能避免全量复制，当然第一次建立连接数据同步是必不可免的，但是其他的情况是可以避免的。  

&emsp; <font color="red">第一次建立连接进行数据同步是全量复制，还有以下几种情况也是全量复制：</font>  

* 从节点发送 psync {runid} {offset} 时，runid 与当前主节点的 runid 不匹配则进行全量复制
* 从节点所需要同步数据的偏移量 offset 不在复制积压缓冲区中，也会进行全量复制。 

#### 2.3.2.2. 部分复制
* 部分重同步是用于处理断线后重复制情况：  
&emsp; 当从服务器在断线后重新连接主服务器时，主服务可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只要接收并执行这些写命令，就可以将数据库更新至主服务器当前所处的状态。  

&emsp; 在 Redis 2.8 开始提供了部分复制，用于处理网络中断的数据同步。  

##### 2.3.2.2.1. 基本概念  
&emsp; 部分复制中设计的几个基本概念：复制偏移量、复制积压缓冲区、运行 ID（runid）。  

###### 2.3.2.2.1.1. 复制偏移量  
&emsp; 主从节点都维护这一个复制偏移量（offset），它代表着当前节点接受数据的字节数，主节点表示接收客户端的字节数，从节点表示接收主节点的字节数，比如从节点接收主节点传来的 N 个字节数据时，从节点的 offset 会增加 N。  
&emsp; 偏移量的作用非常大，它是用来衡量主从节点数据是否一直的唯一标准，如果主从节点的 offset 相等，表明数据一直，否则表明数据不一致。在不一致的情况下，可以根据两个节点的 offset 找出从节点的缺少的那部分数据。比如，主节点的 offset 是 500，从节点的 offset 是 400，那么主节点在进行数据传输时只需要将 401 ~ 500 传递给从节点即可，这就是部分复制。  
&emsp; 从节点通过心跳每秒都会将自身的偏移量告知主节点，所以主节点会保存从节点的偏移量。同时，主节点处理完命令后，会将命令的字节长度累加到自身的偏移量中，如下图：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-41.png)  
&emsp; 从节点每次接受到主节点发送的命令后，也会累加到自身的偏移量中，主节点，如下图  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-42.png)  

###### 2.3.2.2.1.2. 复制积压缓冲区
&emsp; 复制积压缓冲区是一个由主节点维护的缓存队列，它具有如下几个特点：  

* 由主节点维护  
* 固定大小，默认为 1MB，配置参数为：repl-backlog-size  
* 是一个先进先出的队列  

&emsp; 在命令传播节点，主节点除了将写命令传递给从节点，也会将写命令写入到复制积压缓冲区中，当做一个备份，用于在部分复制流程中。由于它是先进先出的队列，且大小固定，所以他只能保存主节点最近执行的写命令，当主从节点的 offset 相差较大时，超出了复制积压缓冲区的范围，则无法进行部分复制，只能进行全量复制了，所以为了能够提高网络中断引起的全量复制，需要认真评估复制积压缓冲区的大小，将其适当调大，比如网络中断时间是 60s,主节点每秒接收的写命令为 100KB，则复制积压缓冲区的平均大小应该为 6MB，所以我们可以将其大小设置为 6MB，甚至是 10MB，来保证绝大多数中断情况下都可以使用部分复制。  

###### 2.3.2.2.1.3. 运行 ID（runid）  
&emsp; 每个 Redis 节点在启动时都会生成一个运行 ID，即 runid，该 ID 用于唯一标识 Redis 节点，它是一个由 40 位随机的十六进制的字符组成的字符串，通过 info server 命令可以查看节点的 runid，如下：  

    127.0.0.1:6379> info server
    ...
    run_id:e88221d68ce96fc28c2f2b3afbf3495ea6de512a
    ...

&emsp; 主从节点在初次建立连接进行全量复制时（从节点发送 psync?-1），主节点会将自己的 runid 告知给从节点，从节点将其保存起来。当主从节点断开重连时，从节点会将这个 runid 发送给主节点，主节点会根据从节点发送的 runid 来判断选择何种复制：  

* 如果从节点发送的 runid 与当前主节点的 runid 一致时，主节点则尝试进行部分复制，当然能不能进行部分复制还要看偏移量是否在复制积压缓冲区  
* 如果从节点发送的 runid 与当前主节点的 runid 不一致时，则进行全量复制  

##### 2.3.2.2.2. 部分复制流程  
&emsp; 当主从节点在命令传播节点发生了网络中断，出现数据丢失情况，则从节点会向主节点请求发送丢失的数据，如果请求的偏移量在复制积压缓冲区中，则主节点就将剩余的数据补发给从节点，保持主从节点数据一致，由于补发的数据一般都会比较小，所以开销相当于全量复制而言也会很小，流程如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-43.png)  
&emsp; 1. 当主从节点出现网络闪退时，如果超过了 repl-timeout 时间，主节点会认为从节点出现故障不可达，打印日志如下：  

    -- master
    2496:M 19 May 2019 10:06:02.970 # Connection with replica 127.0.0.1:6391 lost.
    --slave
    2655:S 19 May 2019 10:21:59.515 * Connecting to MASTER no:6390
    2655:S 19 May 2019 10:21:59.516 # Unable to connect to MASTER: Undefined error: 0

&emsp; 由于主节点没有宕机，所以它依然会响应客户端命令，当然这些命令也不会丢失，都会存储在复制积压缓冲区中，默认 1MB。  

&emsp; 2. 当主从直接恢复连接，从节点再次连接主节点，打印日志如下：  

    2655:S 19 May 2019 10:22:00.082 * REPLICAOF 127.0.0.1:6390 enabled (user request from 'id=6 addr=127.0.0.1:55985 fd=7 name= age=11 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=42 qbuf-free=32726 obl=0 oll=0 omem=0 events=r cmd=slaveof')
    2655:S 19 May 2019 10:22:00.527 * Connecting to MASTER 127.0.0.1:6390
    2655:S 19 May 2019 10:22:00.527 * MASTER <-> REPLICA sync started
    2655:S 19 May 2019 10:22:00.528 * Non blocking connect for SYNC fired the event.
    2655:S 19 May 2019 10:22:00.528 * Master replied to PING, replication can continue...

&emsp; 这里一定要注意：不要关闭从节点然后启动，这样是模拟不出来的，一定是要执行 slaveof no one 命令，因为重启从节点，它的 master_replid 会丢失，在请求的时候因为 runid 不一致而导致全量复制，当然也选择将 slaveof 写入到配置文件中再重启，这样也可以进行部分复制。

&emsp; 3. 当主从建立连接后，由于从节点保存了主节点的 runid 和 offset ，所以只需要发送命令 psync{runid}{offset}即可，从节点打印日志如下：  

    2655:S 19 May 2019 10:22:00.529 * Trying a partial resynchronization (request dfa92dd668e0c6c3447af0d502b6ee4b0b07d75d:2268).

&emsp; 可以看到请求的 runid：dfa92dd668e0c6c3447af0d502b6ee4b0b07d75d，offset：2268  

&emsp; 4. 主节点接受从节点的 psync 命令，会先核对请求的 runid 是否和自身的的 runid 一致，如果一致，说明该从节点复制的当前主节点。然后查看请求的 offset 是否在复制积压缓冲区，如果在则进行部分复制，否则进行全量复制，部分复制回复 +CONTINUE 响应，从节点接受回复后，打印日志如下：  

    2655:S 19 May 2019 10:22:00.530 * Successful partial resynchronization with master.
    2655:S 19 May 2019 10:22:00.530 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.

&emsp; 5. 在进行部分复制时，主节点只需要根据 offset 将复制积压缓冲区的数据补发给从节点即可，主节点打印日志如下：

    2496:M 19 May 2019 10:22:00.529 * Replica 127.0.0.1:6391 asks for synchronization
    2496:M 19 May 2019 10:22:00.529 * Partial resynchronization request from 127.0.0.1:6391 accepted. Sending 183 bytes of backlog starting from offset 2268.

&emsp; 从日志中，可以看出主节点发送了 183 个字节数据给从节点。  

#### 2.3.2.3. psync 命令的执行过程详解  
&emsp; 在 Redis 2.8 以前一直都是通过命令 sync 进行全量复制，但是 Redis 2.8 以后都是通过命令 psync 进行全量复制和部分复制了，所以有必要了解下 psync 命令的执行过程，如下图：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-44.png)  
1. 首先从节点根据当前状态来决定如何调用 psync 命令
    * 如果主从节点从未建立过连接或者之间执行过 slave of none ,则从节点发送命令 psync?-1，向主节点请求全量复制。
    * 如果主从节点建立过连接，则发送命令 psync{runid}{offset} 尝试部分复制，具体是全量还是部分复制，则需要根据主节点的情况来确定
2. 主节点则根据自身情况来做出不同的响应：  
    * 如果主节点的版本低于 2.8 ，则响应 -ERR，从节点接受该回复后发送 sync 进行全量复制  
    * 如果主节点发现请求的命令为 psync?-1, 则判定该从节点是第一次进行连接，则响应 +FULLRESYNC<runid\><offset\>，进行全量复制  
    * 如果主节点比对命令请求的 runid 和自身的 runid 不一致或者一致，但是请求的 offset 不在复制积压缓冲区中，则响应 +FULLRESYNC<runid><offset> 进行全量复制  
    * 如果主节点比对命令请求的 runid 和自身的 runid 一致，且 offset 也在复制积压缓冲区，则响应 +CONTINUE 进行部分复制  

## 2.4. 主从复制应用与问题  
### 2.4.1. 读写分离  
&emsp; 对于Redis的主从模式，可以利用主节点提供写服务，一个或者多个从节点提供读服务，这样就最大化 Redis 的读负载能力。主从架构下的读写分离中的问题：  

* 数据延迟  
* 读到过期数据  
* 从节点故障  

#### 2.4.1.1. 数据延迟  
&emsp; 在命令传送阶段，Redis 主从节点同步命令的过程是异步的，所以势必会导致主从节点的数据不一致性，如果我应用对数据的不一致性接受程度不是很高，则可以从以下几个方面优化：  

* 优化主从节点的网络环境（比如主从节点部署在同机房）  
* 监控从节点的延迟性，如果延迟过大，则通知应用不从该节点读取数据，这种方案需要改造客户端，工作量不小，一般不推荐  

&emsp; 从节点的 slave-serve-stale-data 参数便与其有关，它控制着从节点对读请求的响应情况，如果为 yes（默认值），则从节点可以响应客户端的命令，如果为 NO，则只能响应 info、slaveof 等少数命令。所以如果应用对数据一致性要求较高，则应该将该参数设置为 NO。  

#### 2.4.1.2. 读到过期数据  
&emsp; Redis 内部对过期数据的删除有两种方案：惰性删除、定时删除。  

* 惰性删除：服务器不会主动删除数据，只有当客户端查询某个数据时，服务器判断该数据是否过期，过期则删除  
* 定时删除：服务器内部维护着一个定时任务，会定时删除过期的数据  

&emsp; 在主从复制的架构模式下，所有的写操作都发生在主节点，删除数据时也是在主节点删除，然后将删除命令同步给从节点，由于主从节点的延迟性，主节点删除了某些数据，从节点不一定也删除了，所以在从节点是比较容易读到过期的数据。但是在 Redis 3.2 版本中，增加了对数据是否过期的判断，如果读到的数据过期了，则不会返回给客户端了，所以将 Redis 升级到 3.2 版本后即可解决该问题。  

#### 2.4.1.3. 从节点故障  
&emsp; 如果从节点出现故障了，在没有哨兵的情况下，客户端是无法进行切换的，所以需要对客户端进行改造，它内部需要维护一个可用的 Redis 节点列表，当某个节点出现故障不可用后，立刻切换到其他可用节点，当然这种工作量会比较大，得不偿失。  


### 2.4.2. 规避全量复制  
&emsp; 全量复制要经过如下几个过程：建立连接 ---> 生成 RDB 文件 ---> 发送 RDB 文件 ---> 清空旧数据 ---> 加载新数据，五个过程，工作量巨大，是一个非常消耗资源的操作，所以我们需要尽可能的规避。有以下几种情况会发生全量复制：  

* 第一次复制
* 节点运行 ID 不匹配
* 复制偏移量 offset 不在复制积压缓冲区中

#### 2.4.2.1. 第一次复制  
&emsp; 这种情况是无法避免的，所以如果要对数据量较大且流量较高的主节点添加从节点的话，最好是选择在低峰值进行操作。

#### 2.4.2.2. 节点运行 ID 不匹配  
&emsp; 主节点重启时，runid 会发生改变，在进行复制的时候，发现主从节点的 runid 不一致，则进行全量复制，对于这种情况一般都应该在架构上面规避，比如提供故障转移的功能。还有一种情况就是修改了主节点的配置，需要重启才能够生效，这个时候如果重启主节点发生全量复制就得不偿失了，可以选择安全重启的方式（debug reload），在这种情况，主节点重启的 runid 是不会发生改变的。  

#### 2.4.2.3. 复制偏移量 offset 不在复制积压缓冲区中  
&emsp; 当主从节点因网络中断而重新连接，从节点会发送 psync 命令进行部分复制请求，主节点除了校验 runid 是否一致外还会判断请求的 offset 是否在缓冲区中，如果不在这进行全量复制。复制积压缓冲区默认大小为 1MB，这对于高流量的主节点而言势必显得有点儿小了，所以为了避免全量复制，我们需要根据中断时长来调整复制积压缓冲区的大小，调整为 复制积压缓冲区大小>平均中断时长*平均写命令字节数，这样就可以避免因为复制积压缓冲区不足而导致的全量复制。  

### 2.4.3. 规避复制风暴  
&emsp; 复制风暴指的是大量从节点对同一主节点或者同一台服务器的多个主节点短时间内发起全量复制的过程。复制风暴会导致主节点消耗大量的 CPU、内存、宽带，所以需要尽可能的规避复制风暴。  

#### 2.4.3.1. 单一主节点  
&emsp; 这种情况一般都发生在一个主节点挂载了多个从节点，所以规避的方案也比较简单：  

* 减少挂载的从节点  
* 将架构调整为树状结构，增加中间层，但是这样导致的后果是中间层越多，后面节点的数据延迟就越高，同时也增加了运维的难度  

#### 2.4.3.2. 单一机器  
&emsp; Redis 的单线程运行的，所以会存在一台机器上面部署多个 Redis 服务，同时也有多个是主节点，所以规避方案：  

* 将主节点分散在多台不同的机器上  
* 当主节点所在机器故障后提供故障转移机制，避免机器回复后进行密集的全量复制  

<!-- 
主从复制的相关配置
https://mp.weixin.qq.com/s/OdvVn5pBG3Qlz9x6jttMXQ
-->

---
# 3. 哨兵模式  
&emsp; 主从模式弊端：当Master宕机后，Redis集群将不能对外提供写入操作，需要手动将一个从节点晋升为主节点，同时需要修改应用方的主节点地址， 还需要命令其他从节点去复制新的主节点， 整个过程都需要人工干预。在 Redis 2.8 提供比较完善的解决方案：Redis Sentinel。Redis Sentinel 是一个能够自动完成故障发现和故障转移并通知应用方，从而实现真正的高可用的分布式架构。下面是Redis官方文档对于哨兵功能的描述：  
<!-- 哨兵，英文名 Sentinel，是一个分布式系统，用于对主从结构中的每一台服务器进行监控，当主节点出现故障后通过投票机制来挑选新的主节点，并且将所有的从节点连接到新的主节点上。-->
* 监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。  
* 自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。  
* 配置提供者（Configuration provider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。  
* 通知（Notification）：哨兵可以将故障转移的结果发送给客户端。  

&emsp; <font color="red">监控和自动故障转移使得 Sentinel 能够完成主节点故障发现和自动转移，配置提供者和通知则是实现通知客户端主节点变更的关键。</font>  

## 3.1. 架构  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-45.png)  
&emsp; 典型的哨兵架构图如上图所示，它主要包括两个部分：哨兵节点和数据节点。  

* 哨兵节点：哨兵系统由若干个哨兵节点组成。其实哨兵节点是一个特殊的 Redis 节点，<font color="red">哨兵节点不存储数据的和仅支持部分命令。配置哨兵时配置为单数。</font>  
* 数据节点：由主节点和从节点组成的数据节点。  

![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-27.png)  

&emsp; Redis Sentinel部署架构主要包括两部分：Redis Sentinel集群和Redis数据集群。  
&emsp; 其中Redis Sentinel集群是由若干Sentinel节点组成的分布式集群，可以实现故障发现、故障自动转移、配置中心和客户端通知。Redis Sentinel的节点数量要满足2n+1（n>=1）的奇数个。   
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-28.png)  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-29.png)  

## 3.2. 部署及故障转移演示    
......

## 3.3. 哨兵原理  
### 3.3.1. 心跳检查  
&emsp; Sentinel 通过三个定时任务来完成对各个节点的发现和监控，这是保证 Redis 高可用的重要机制。  
1. 每隔 10 秒，每个 Sentinel 节点会向已知的主从节点发送 info 命令获取最新的主从架构。下图是 info 命令的响应。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-46.png)  
&emsp; Sentinel 节点通过解析响应信息，就可以知道当前 Redis 数据节点的最新拓扑结构。如果是新增的节点，Sentinel 就会与其建立连接。  
2. 每隔 2 秒，Sentinel 节点都会向主从节点的 _sentinel_:hello 频道发送自己的信息。目的有两个：  

    * 发现新的 Sentinel 节点  
    * Sentinel 节点之间交换主节点的状态，作为后面客观下线以及领导者选择的依据。  

    &emsp; 发送的消息内容格式为：  

        <哨兵地址>,<哨兵端口>,<哨兵的运行ID>,<哨兵的配置版本>,
        <主数据库的名称>,<主数据库的地址>,<主数据库的端口>,<主数据库的配置版本>

3. 每隔一秒，哨兵会每个主从节点、Sentinel 节点发送 PING 命令。该定时任务是哨兵心跳机制中的核心，它涉及到 Redis 数据节点的运行状态监控，哨兵领导者的选举等细节操作。当哨兵节点发送 PING 命令后，若超过 down-after-milliseconds 后，当前哨兵节点会认为该节点主观下线。  

### 3.3.2. 主观下线、客观下线  
#### 3.3.2.1. 主观下线  
&emsp; 在第三个定时任务中，每个一秒 Sentinel 节点会向每个 Redis 数据节点发送 PING 命令，若超过 down-after-milliseconds 设定的时间没有收到响应，则会对该节点做失败判定，这种行为叫做 主观下线。因为该行为是当前节点的一家之言，所以会存在误判的可能。  

#### 3.3.2.2. 客观下线  
&emsp; 当 Sentinel 节点判定一个主节点为主观下线后，则会通过 sentinelis-master-down-by-addr 命令询问其他 Sentinel 节点对该主节点的状态，当有超过 <quorunm\> 个 Sentinel 节点认为主节点存在问题，这时该 Sentinel节点会对主节点做客观下线的决定。  
&emsp; 这里有一点需要注意的是，客观下线是针对主机节点，如果主观下线的是从节点或者其他 Sentinel 节点，则不会进行后面的客观下线和故障转移了。  

### 3.3.3. Sentinel 选举  
&emsp; 假如一个 Sentinel 节点完成了主节点的客观下线，那么是不是就可以立刻进行故障转移呢？显然不是，因为 Redis 的故障转移工作只需要一个 Sentinel 节点来完成，所以会有一个选举的过程，选举出来一个领导者来完成故障转移工作。Redis 节点采用 Raft 算法来完成领导者写选举。    

&emsp; Sentinel 选举的主要流程：
1. 每一个做主观下线的 Sentinel 节点都有成为领导者的可能，他们会想其他 Sentinel 节点发送 sentinelis-master-down-by-addr ，要求将它设置为领导者。  
2. 收到命令的 Sentinel 节点如果没有同意其他 Sentinel 节点发送的命令，则会同意该请求，否则拒绝。  
3. 如果该 Sentinel 节点发现自己得到的票数已经超过半数且超过 <quorum\>，那么他将成为领导者。  
4. 如果该过程有多个 Sentinel 成为领导者，那么将等待一段时间重新进行选择，直到有且只有一个 Sentinel 节点成为领导者为止。  

&emsp; 加入有 A、B、C、D 四个节点构成 Sentinel 集群。加入 A 率先完成客观下线，则 A 会向 B、C、D 发出成为领导者的申请，由于 B、C、D 没有同意过其他 Sentinel 节点，所以会将投票给 A，A 得到三票。B 则向 A、C、D 发起申请，由于 C、D 已经同意了 A，所以会拒绝，但是他会得到 A 的同意，所以 B 得到一票，同理 C、D 得到零票，如下图：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-47.png)  
&emsp; 所以 A 会成为领导者，进行故障转移工作。一般来说，哨兵选择的过程很快，谁先完成客观下线，一般就能成为领导者。  

### 3.3.4. 故障转移  
&emsp; 当某个 Sentinel 节点通过选举成为了领导者，则他就要承担起故障转移的工作，其具体步骤如下：  
1. 从从节点列表中选择一个节点作为新的主节点，选择的策略如下：  

    * 过滤掉不健康的节点（主观下线、断线），5 秒内没有回复过 Sentinel 节点 ping 响应、与主节点失联超过 down-after-milliseconds*10秒  
    * 选择优先级最高级别的节点，如果不存在则继续  
    * 选择复制偏移量最大的节点（数据最完整），存在则返回，不存在则继续  
    * 选择 runid 最小的节点  
2. 在新的主节点上执行 slaveofnoone，让其变成主节点  
3. 向剩余的从节点发送命令,让它们成为新主节点的从节点  

<!-- 
 3.4. Sentinel（哨兵）进程的工作方式：  
1. 每个Sentinel（哨兵）进程以每秒钟一次的频率向整个集群中的Master主服务器，Slave从服务器以及其他Sentinel（哨兵）进程发送一个 PING 命令。  
2. 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被 Sentinel（哨兵）进程标记为主观下线（SDOWN）。  
3. 如果一个Master主服务器被标记为主观下线（SDOWN），则正在监视这个Master主服务器的所有 Sentinel（哨兵）进程要以每秒一次的频率确认Master主服务器的确进入了主观下线状态。  
4. 当有足够数量的 Sentinel（哨兵）进程（大于等于配置文件指定的值）在指定的时间范围内确认Master主服务器进入了主观下线状态（SDOWN）， 则Master主服务器会被标记为客观下线（ODOWN）。  
5. 在一般情况下， 每个 Sentinel（哨兵）进程会以每 10 秒一次的频率向集群中的所有Master主服务器、Slave从服务器发送 INFO 命令。
6. 当Master主服务器被 Sentinel（哨兵）进程标记为客观下线（ODOWN）时，Sentinel（哨兵）进程向下线的 Master主服务器的所有 Slave从服务器发送 INFO 命令的频率会从 10 秒一次改为每秒一次。  
7. 若没有足够数量的 Sentinel（哨兵）进程同意 Master主服务器下线， Master主服务器的客观下线状态就会被移除。若 Master主服务器重新向 Sentinel（哨兵）进程发送 PING 命令返回有效回复，Master主服务器的主观下线状态就会被移除。  
-->


----
# 4. 分片模式  
&emsp; 分片(sharding)是将数据拆分到多个Redis实例的过程，这样每个实例将只包含所有键的子集，这种方法在解决某些问题时可以获得线性级别的性能提升。  
&emsp; 根据执行分片的位置，可以分为三种分片方式：  

* 客户端分片：在客户端实现相关的逻辑，例如用取模或者一致性哈希对 key 进行分片，查询和修改都先判断 key 的路由。  
* 代理分片：把做分片处理的逻辑抽取出来，运行一个独立的代理服务，客户端连接到 这个代理服务，代理服务做请求的转发。  
* 服务器分片：官方Redis Cluster。  

## 4.1. 基于客户端分片  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-18.png)  
&emsp; Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是基于哈希算法，根据Redis数据的key的哈希值对数据进行分片，将数据映射到各自节点上。  
&emsp; 优点在于实现简单，缺点在于当Redis集群调整，每个客户端都需要更新调整。  
&emsp; ***在redis3.0版本之前的版本，可以通过redis客户端做sharding分片，比如jedis实现的ShardedJedisPool。***  

## 4.2. 基于代理服务器分片
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-19.png)  
&emsp; 客户端发送请求到独立部署代理组件，代理组件解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端。  
&emsp; 优点在于透明接入，容易集群扩展，缺点在于多了一层代理转发，性能有所损耗。  
&emsp; 典型的代理分区方案有 Twitter 开源的 Twemproxy 和国内的豌豆荚开源的 Codis。  

## 4.3. Redis Cluster集群  
&emsp; Redis Cluster是在3.0版本正式推出的高可用集群方案。  
&emsp; Redis Cluster相比Redis Sentinel，Redis Cluster方案不需要额外部署Sentinel集群，而是通过集群内部通信实现集群监控，故障时主从切换；同时，支持内部基于哈希实现数据分片，支持动态水平扩容。  
&emsp; Redis Cluster相比Codis，它是去中心化的，客户端可以连接到任意一个可用节点。  

*  高性能  
    * 采用了异步复制机制，向某个节点写入数据时，无需等待其它节点的写数据响应。  
    * 去中心化，无中心代理节点，每个节点保存数据和整个集群状态，每个节点都和其他所有节点连接，将客户端直接重定向到拥有数据的节点。  
    * 对于N个Master节点的Cluster ，整体性能理论上相当于单个Redis的性能的N倍。  
* 高可用
    * 采用了主从复制的机制，Master节点失效时Slave节点自动提升为Master节点。如果Cluster中有N个Master节点，每个Master拥有1个Slave节点，那么这个Cluster的失效概率为1/(2*N-1)，可用概率为 1-1/(2*N-1)。  
* 高可扩展  
    * 可支持多达1000个服务节点。随时可以向 Cluster 中添加新节点，或者删除现有节点。Cluster中每个节点都与其它节点建立了相互连接。  


&emsp; 优势：  
1. 无中心架构。 
2. 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。 
3. 可扩展性，可线性扩展到 1000 个节点（官方推荐不超过 1000 个），节点可动 态添加或删除。 
4. 高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副 本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制 完成 Slave 到 Master 的角色提升。 
5. 降低运维成本，提高系统的扩展性和可用性。


&emsp; 不足：  
1. Client 实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时 更新，提高了开发难度，客户端的不成熟影响业务的稳定性。 
2. 节点会因为某些原因发生阻塞（阻塞时间大于 clutser-node-timeout），被判断 下线，这种 failover 是没有必要的。 
3. 数据通过异步复制，不保证数据的强一致性。
4. 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。

### 4.3.1. 架构  
&emsp; 服务器节点：3主3从，最少6个节点。其 Redis Cluster架构图如下：  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-30.png)  

### 4.3.2. 部署  
...

### 4.3.3. 原理  
#### 4.3.3.1. 数据分布  
##### 4.3.3.1.1. Redis数据分区  
&emsp; Redis集群并没有使用传统的一致性哈希来分配数据，而是采用哈希槽(hash slot)的方式来分配数据。Redis Cluster默认分配了16384（2∧14）个slot，set一个key时，redis采用CRC16算法进行键-槽（key->slot）之间的映射。  
&emsp; HASH_SLOT（key）= CRC16(key) % 16384  
&emsp; 其中 CRC16(key) 语句用于计算键key的CRC16校验和。key经过公式计算后得到所对应的哈希槽，而哈希槽被某个主节点管理，从而确定key在哪个主节点上存取。  
&emsp; 分区好处：RedisCluster将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞，所以无论是添加新节点还是移除已存在节点，又或者改变某个节点包含的哈希槽数量，都不会造成集群下线。从而保证集群的可用性，使得用户可以很容易地向集群中添加或者删除节点。  
&emsp; 示例：现在三个主节点分别是：A, B, C三个节点，它们可以是一台机器上的三个端口，也可以是三台不同的服务器。那么，采用哈希槽 (hash slot)的方式来分配16384个slot 的话，它们三个节点分别承担的slot区间是：节点A覆盖0－5460；节点B覆盖5461－10922；节点C覆盖10923－16383。  
&emsp; 获取数据：如果存入一个值，按照redis cluster哈希槽的算法CRC16('key')384 = 6782。 那么就会把这个key 的存储分配到B上了。同样，当连接(A,B,C)任何一个节点想获取'key'这个key时，也会这样的算法，然后内部跳转到B节点上获取数据。  
&emsp; 新增一个主节点：新增一个节点D，redis cluster的这种做法是从各个节点的前面各拿取一部分slot到D上，会在接下来的实践中实验。大致就会变成这样：节点A覆盖1365-5460；节点B覆盖6827-10922；节点C覆盖12288-16383；节点D覆盖0-1364,5461-6826,10923-12287。  
&emsp; 同样删除一个节点也是类似，移动完成后就可以删除这个节点了。  

##### 4.3.3.1.2. 集群功能限制  
&emsp; Redis集群相对单机在功能上存在一些限制，需要开发人员提前了解， 在使用时做好规避。限制如下：  
1. key批量操作支持有限。如mset、mget，目前只支持具有相同slot值的 key执行批量操作。对于映射为不同slot值的key由于执行mget、mget等操作可 能存在于多个节点上因此不被支持。  
2. key事务操作支持有限。同理只支持多key在同一节点上的事务操 作，当多个key分布在不同的节点上时无法使用事务功能。   
3. key作为数据分区的最小粒度，因此不能将一个大的键值对象如 hash、list等映射到不同的节点。   
4. 不支持多数据库空间。单机下的Redis可以支持16个数据库，集群模 式下只能使用一个数据库空间，即db0。  
5. 复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复 制结构。  

#### 4.3.3.2. 集群伸缩  
##### 4.3.3.2.1. 伸缩原理  
&emsp; Redis集群提供了灵活的节点扩容和收缩方案。在不影响集群对外服务 的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容。  
&emsp; Redis集群可以实现对节点的灵活上下线控制。其中原理可抽象为槽和对应数据在不同节点之间灵活移动。    

##### 4.3.3.2.2. 扩容集群  
扩容是分布式存储最常见的需求，Redis集群扩容操作可分为如下步骤：
1. 准备新节点。 
2. 加入集群。 
3. 迁移槽和数据。

##### 4.3.3.2.3. 收缩集群  


#### 4.3.3.3. 请求路由，客户端重定向  
&emsp; Redis集群对客户端通信协议做了比较大的修改， 为了追求性能最大化，并没有采用代理的方式而是采用客户端直连节点的方式。  

##### 4.3.3.3.1. 请求重定向  
&emsp; 在集群模式下，Redis接收任何键相关命令时首先计算键对应的槽，再 根据槽找出所对应的节点，如果节点是自身，则处理键命令；否则回复MOVED重定向错误，通知客户端请求正确的节点。这个过程称为MOVED重定向。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-48.png)  

##### 4.3.3.3.2. Smart客户端  
&emsp; 大多数开发语言的Redis客户端都采用Smart客户端支持集群协议，客户 端如何选择见：http://redis.io/clients，从中找出符合自己要求的客户端类 库。Smart客户端通过在内部维护slot→node的映射关系，本地就可实现键到 节点的查找，从而保证IO效率的最大化，而MOVED重定向负责协助Smart客 户端更新slot→node映射。  
&emsp; Jedis为Redis Cluster提供了Smart客户端，对应的类是JedisCluster。  

##### 4.3.3.3.3. ASK重定向  
&emsp; Redis集群支持在线迁移槽（slot）和数据来完成水平伸缩，当slot对应 的数据从源节点到目标节点迁移过程中，客户端需要做到智能识别，保证键 命令可正常执行。例如当一个slot数据从源节点迁移到目标节点时，期间可 能出现一部分数据在源节点，而另一部分在目标节点。  
&emsp; 当出现上述情况时，客户端键命令执行流程将发生变化，如下所示： 
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-49.png)  
1. 客户端根据本地slots缓存发送命令到源节点，如果存在键对象则直 接执行并返回结果给客户端。 
2. 如果键对象不存在，则可能存在于目标节点，这时源节点会回复 ASK重定向异常。格式如下：（error）ASK{slot}{targetIP}：{targetPort}。 
3. 客户端从ASK重定向异常提取出目标节点信息，发送asking命令到目 标节点打开客户端连接标识，再执行键命令。如果存在则执行，不存在则返 回不存在信息。  

&emsp; ASK与MOVED虽然都是对客户端的重定向控制，但是有着本质区别。 ASK重定向说明集群正在进行slot数据迁移，客户端无法知道什么时候迁移 完成，因此只能是临时性的重定向，客户端不会更新slots缓存。但是 MOVED重定向说明键对应的槽已经明确指定到新的节点，因此需要更新 slots缓存。  

#### 4.3.3.4. 故障转移  
##### 4.3.3.4.1. 故障发现  
&emsp; 当集群内某个节点出现问题时，需要通过一种健壮的方式保证识别出节 点是否发生了故障。Redis集群内节点通过ping/pong消息实现节点通信，消息不但可以传播节点槽信息，还可以传播其他状态如：主从状态、节点故障 等。因此故障发现也是通过消息传播机制实现的，主要环节包括：主观下线 （pfail）和客观下线（fail）。   

* 主观下线：指某个节点认为另一个节点不可用，即下线状态，这个状 态并不是最终的故障判定，只能代表一个节点的意见，可能存在误判情况。   
* 客观下线：指标记一个节点真正的下线，集群内多个节点都认为该节 点不可用，从而达成共识的结果。如果是持有槽的主节点故障，需要为该节 点进行故障转移。  

##### 4.3.3.4.2. 故障恢复  
&emsp; 故障节点变为客观下线后，如果下线节点是持有槽的主节点则需要在它 的从节点中选出一个替换它，从而保证集群的高可用。下线主节点的所有从 节点承担故障恢复的义务，当从节点通过内部定时任务发现自身复制的主节 点进入客观下线时，将会触发故障恢复流程。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-50.png)  

### 4.3.4. 集群运维  
#### 4.3.4.1. 集群倾斜  

&emsp; 集群倾斜指不同节点之间数据量和请求量出现明显差异，这种情况将加 大负载均衡和开发运维的难度。因此需要理解哪些原因会造成集群倾斜，从 而避免这一问题。   
1. 数据倾斜  
&emsp; 数据倾斜主要分为以下几种： 
* 节点和槽分配严重不均。 
* 不同槽对应键数量差异过大。 
* 集合对象包含大量元素。 
* 内存相关配置不一致。

2. 请求倾斜  
&emsp; 集群内特定节点请求量/流量过大将导致节点之间负载不均，影响集群 均衡和运维成本。常出现在热点键场景，当键命令消耗较低时如小对象的 get、set、incr等，即使请求量差异较大一般也不会产生负载严重不均。但是 当热点键对应高算法复杂度的命令或者是大对象操作如hgetall、smembers等，会导致对应节点负载过高的情况。避免方式如下：  
* 合理设计键，热点大集合对象做拆分或使用hmget替代hgetall避免整 体读取。
* 不要使用热键作为hash_tag，避免映射到同一槽。 
* 对于一致性要求不高的场景，客户端可使用本地缓存减少热键调用。  



<!-- 
-----
 5. 集群优雅扩容  
&emsp; 程序如果能够知道Redis集群地址产生了变化，重新设置一下jedis客户端的连接配置。现在的问题就是如何知道Redis集群地址发生了改变？
可以采用把Redis的集群地址配置在zookeeper中，应用在启动的时候，获取zk上的集群地址的值，进行初始化。如果想要改变集群地址，要在zk上面进行设置。  
&emsp; zk重要的特性就是监听特性，节点发生变化，就会立刻把变化发送给应用，从而应用获取到值，重新设置jedis客户端连接。  
![image](https://gitee.com/wt1814/pic-host/raw/master/images/microService/Redis/redis-26.png)  
-->


